{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LANGCHAIN_TRACING_V2'] = \"true\"\n",
    "os.environ['LANGCHAIN_PROJECT'] = \"lg-reflexion-agents\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor\n",
    "- Tools/tool execution\n",
    "- Initial responder: generate an initial response (and self-reflection)\n",
    "- Revisor: re-respond (and reflec) based on previous reflections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Contruct tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import List\n",
    "\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper\n",
    "\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, ToolMessage\n",
    "from langgraph.prebuilt.tool_executor import ToolExecutor, ToolInvocation\n",
    "\n",
    "from langchain.output_parsers.openai_tools import (\n",
    "    JsonOutputToolsParser,\n",
    "    PydanticToolsParser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools\n",
    "search = TavilySearchAPIWrapper()\n",
    "tavily_tool = TavilySearchResults(api_wrapper=search, max_results=5)\n",
    "\n",
    "# helper class for running tools, it takes in an agent action and calls that tool and returns the result\n",
    "tool_executor = ToolExecutor([tavily_tool])\n",
    "parser = JsonOutputToolsParser(return_id=True) # parsing tool messages for the execution / invocation\n",
    "\n",
    "# helper function to run the tool\n",
    "def execute_tools(state: List[BaseMessage]) -> List[BaseMessage]:\n",
    "    tool_invocation: AIMessage = state[-1]\n",
    "    parsed_tool_calls = parser.invoke(tool_invocation)\n",
    "    ids = []\n",
    "    tool_invocations = []\n",
    "    for parsed_call in parsed_tool_calls:\n",
    "        for query in parsed_call[\"args\"][\"search_queries\"]:\n",
    "            tool_invocations.append(\n",
    "                ToolInvocation(\n",
    "                    tool=\"taivly_search_results_json\",\n",
    "                    tool_input=query\n",
    "                )\n",
    "            )\n",
    "            ids.append(parsed_call[\"id\"])\n",
    "    \n",
    "    outputs = tool_executor.batch(tool_invocation)\n",
    "    outputs_map = defaultdict(dict)\n",
    "    for id_, output, invocation in zip(ids, outputs, tool_invocations):\n",
    "        outputs_map[id_][invocation.tool_input] = output\n",
    "    \n",
    "    return [\n",
    "        ToolMessage(content=json.dumps(query_outputs), tool_call_id=id_)\n",
    "        for id_, query_outputs in outputs_map.items()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Initial Responder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, ValidationError\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith import traceable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the prompt template for the actor - the responder - the expert researcher\n",
    "actor_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are expert researcher.\n",
    "Current time: {time}\n",
    "\n",
    "1. {first_instruction}\n",
    "2. Reflect and critique your answer. Be severe to maximize improvement.\n",
    "3. Recommend search queries to research information and improve your answer.\"\"\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\"system\", \"Answer the user's question above using the required format.\")\n",
    "    ]\n",
    ").partial(\n",
    "    time=lambda: datetime.datetime.now().isoformat(),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "class Reflection(BaseModel):\n",
    "    \"\"\"Explicitly prompting the criticism to generate both missing and superfluous \n",
    "    aspects of its response.\n",
    "    \"\"\"\n",
    "\n",
    "    missing: str = Field(description=\"Critique of what is missing.\")\n",
    "    superfluous: str = Field(description=\"Critique of what is superfluous.\")\n",
    "\n",
    "class AnswerQuestion(BaseModel):\n",
    "    \"\"\"Answer the question.\"\"\"\n",
    "\n",
    "    answer: str = Field(description=\"~250 word detailed answer to the question.\")\n",
    "    reflection: Reflection = Field(description=\"Your reflection on the initial answer.\")\n",
    "    search_queries: List[str] = Field(\n",
    "        description=\"1-3 search queries for researching improvements to address the critique of your current answer.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4-turbo-preview\")\n",
    "\n",
    "initial_answer_chain = actor_prompt_template.partial(\n",
    "    first_instruction=\"Provide a detailed ~250 word answer.\"\n",
    ") | llm.bind_tools(tools=[AnswerQuestion], tool_choice=\"AnswerQuestion\")\n",
    "\n",
    "validator = PydanticToolsParser(tools=[AnswerQuestion])\n",
    "\n",
    "\n",
    "class ResponderWithRetries:\n",
    "\n",
    "    def __init__(self, runnable, validator):\n",
    "        self.runnable = runnable\n",
    "        self.validator = validator\n",
    "\n",
    "    @traceable\n",
    "    def respond(self, state: List[BaseMessage]):\n",
    "        response = []\n",
    "        for attempt in range(3):\n",
    "            try:\n",
    "                response = self.runnable.invoke({\"messages\": state})\n",
    "                self.validator.invoke(response)\n",
    "                return response\n",
    "            except ValidationError as e:\n",
    "                print(f\"Validation error: {e}\")\n",
    "                print(f\"Retrying attempt {attempt}\")\n",
    "                state = state + [HumanMessage(content=repr(e))]\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_responder = ResponderWithRetries(\n",
    "    runnable=initial_answer_chain,\n",
    "    validator=validator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_question = \"What is the importance of continuous batching in LLMs serving?\"\n",
    "initial = first_responder.respond([HumanMessage(content=example_question)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'AnswerQuestion',\n",
       "  'args': {'answer': 'Continuous batching in the context of serving Large Language Models (LLMs) refers to the process of dynamically grouping incoming requests into batches for parallel processing. This technique is crucial for several reasons:\\n\\n1. **Efficiency**: By processing requests in batches, LLMs can leverage hardware acceleration (e.g., GPUs or TPUs) more effectively, leading to faster response times and lower per-request processing costs. It optimizes resource utilization by ensuring that the computational power is not idling between single requests.\\n\\n2. **Scalability**: Continuous batching allows for the system to adjust dynamically to varying loads, making it easier to handle peak traffic periods. This scalability is essential for maintaining performance and availability without over-provisioning resources.\\n\\n3. **Cost-effectiveness**: Since LLMs can be resource-intensive, optimizing their operation through batching reduces operational costs. By improving the throughput and reducing the latency, organizations can serve more users with the same or even reduced infrastructure, leading to a better cost-to-performance ratio.\\n\\n4. **Quality of Service (QoS)**: Continuous batching helps maintain a consistent quality of service by managing and reducing the variance in response times. This is particularly important for applications requiring real-time or near-real-time responses.\\n\\nIn summary, continuous batching is a key operational technique for deploying LLMs efficiently and effectively, ensuring that they can meet the demands of modern applications while remaining cost-effective.',\n",
       "   'reflection': {'missing': \"The answer could benefit from a real-world example illustrating the impact of continuous batching on an LLM's performance. Including such an example would make the explanation more tangible and relatable.\",\n",
       "    'superfluous': 'The answer is quite comprehensive in terms of covering the theoretical aspects of continuous batching. However, the inclusion of more technical details or specific strategies for implementing continuous batching could be considered superfluous for some readers, especially those not familiar with LLM operational intricacies.'},\n",
       "   'search_queries': ['continuous batching in LLMs real-world examples',\n",
       "    'LLMs continuous batching performance impact',\n",
       "    'LLMs continuous batching implementation strategies']},\n",
       "  'id': 'call_zEad4TgIRKnVAmI4DiyAYnRu'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed = parser.invoke(initial)\n",
    "parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Revision\n",
    "- The second part of the Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "revise_instructions = \"\"\"Revise your previous answer using the new information.\n",
    "    - You should use the previous critique to add important information to your answer.\n",
    "        - You MUST include numerical citations in your revised answer to ensure it can be verified.\n",
    "        - Add a \"References\" section to the bottom of your answer (which does not count towards the word limit). In form of:\n",
    "                - [1] https://www.example.com\n",
    "                - [2] https://www.example.com\n",
    "    - You should use the previous critique to remove superfluous information from your answer and make SURE it is not more than 250 words.\n",
    "\"\"\"\n",
    "\n",
    "# Extend the initial answer schema to include the references\n",
    "class ReviseAnswer(AnswerQuestion):\n",
    "    \"\"\"Revise your original answer to the question.\"\"\"\n",
    "    \n",
    "    references: List[str] = Field(description=\"Citations motivating your updated answer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "revision_chain = actor_prompt_template.partial(\n",
    "    first_instruction=revise_instructions\n",
    ") | llm.bind_tools(tools=[ReviseAnswer], tool_choice=\"ReviseAnswer\")\n",
    "\n",
    "revision_validator = PydanticToolsParser(tools=[ReviseAnswer])\n",
    "\n",
    "revisor = ResponderWithRetries(runnable=revision_chain, validator=revision_validator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "revised = revisor.respond(\n",
    "    [\n",
    "        HumanMessage(content=\"\"),\n",
    "        initial,\n",
    "        ToolMessage(\n",
    "            tool_call_id=initial.additional_kwargs[\"tool_calls\"][0][\"id\"],\n",
    "            content=json.dumps(\n",
    "                tavily_tool.invoke(str(parsed[0][\"args\"][\"search_queries\"]))\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'ReviseAnswer',\n",
       "  'args': {'answer': \"Continuous batching in the context of serving Large Language Models (LLMs) significantly enhances their efficiency, scalability, cost-effectiveness, and Quality of Service (QoS). This process dynamically groups incoming requests into batches for parallel processing, leveraging hardware acceleration more effectively [1]. A real-world example of continuous batching's impact is observed in vLLM, where it was shown to reduce latency by immediately injecting new requests when possible and enabling advanced memory optimizations. These optimizations increased the Queries Per Second (QPS) that the serving system could handle before becoming saturated, significantly improving over static batching [5].\\n\\nEfficient resource utilization is crucial for maintaining performance and availability without over-provisioning resources, especially during peak traffic periods. By optimizing throughput and reducing latency, organizations can serve more users with the same or reduced infrastructure, leading to a better cost-to-performance ratio. Continuous batching also helps manage and reduce the variance in response times, ensuring a consistent quality of service for applications requiring real-time or near-real-time responses.\",\n",
       "   'reflection': {'missing': \"The initial answer lacked a specific real-world example to illustrate the impact of continuous batching on an LLM's performance, which could have made the explanation more tangible and relatable.\",\n",
       "    'superfluous': 'The initial answer included a broad explanation of the benefits of continuous batching without concrete examples or details on its direct impact on LLM performance, making it somewhat abstract for readers seeking practical insights.'},\n",
       "   'search_queries': ['continuous batching in LLMs real-world examples',\n",
       "    'LLMs continuous batching performance impact',\n",
       "    'LLMs continuous batching implementation strategies'],\n",
       "   'references': ['https://datamokotow.com/optimizing-large-language-model-inference-with-continuous-batching',\n",
       "    'https://medium.com/@yohoso/llm-inference-optimisation-continuous-batching-2d66844c19e9',\n",
       "    'https://arxiv.org/pdf/2401.17644.pdf',\n",
       "    'https://insujang.github.io/2024-01-07/llm-inference-continuous-batching-and-pagedattention/',\n",
       "    'https://www.anyscale.com/blog/continuous-batching-llm-inference']},\n",
       "  'id': 'call_gxbeSWegVhYuezMeitMQHiWp'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed = parser.invoke(revised)\n",
    "parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, MessageGraph\n",
    "\n",
    "MAX_ITERATIONS = 5\n",
    "builder = MessageGraph()\n",
    "builder.add_node(\"draft\", first_responder.respond)\n",
    "builder.add_node(\"execute_tools\", execute_tools)\n",
    "builder.add_node(\"revise\", revisor.respond)\n",
    "# draft -> execute_tools\n",
    "builder.add_edge(\"draft\", \"execute_tools\")\n",
    "# execute_tools -> revise\n",
    "builder.add_edge(\"execute_tools\", \"revise\")\n",
    "\n",
    "# Define looping logic:\n",
    "\n",
    "\n",
    "def _get_num_iterations(state: List[BaseMessage]):\n",
    "    i = 0\n",
    "    for m in state[::-1]:\n",
    "        if not isinstance(m, (ToolMessage, AIMessage)):\n",
    "            break\n",
    "        i += 1\n",
    "    return i\n",
    "\n",
    "\n",
    "def event_loop(state: List[BaseMessage]) -> str:\n",
    "    # in our case, we'll just stop after N plans\n",
    "    num_iterations = _get_num_iterations(state)\n",
    "    if num_iterations > MAX_ITERATIONS:\n",
    "        return END\n",
    "    return \"execute_tools\"\n",
    "\n",
    "\n",
    "# revise -> execute_tools OR end\n",
    "builder.add_conditional_edges(\"revise\", event_loop)\n",
    "builder.set_entry_point(\"draft\")\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = graph.stream(\n",
    "    [HumanMessage(content=\"What is the importance of continuous batching in LLMs serving?\")]\n",
    ")\n",
    "for i, step in enumerate(events):\n",
    "    \n",
    "    node, output = next(iter(step.items()))\n",
    "    print(f\"## {i+1}. {node}\")\n",
    "    print(str(output) + \" ...\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parser.invoke(step[END][-1])[0][\"args\"][\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
