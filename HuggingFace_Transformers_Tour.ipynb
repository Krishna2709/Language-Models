{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krish\\miniconda3\\envs\\llmsys\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src: https://www.msn.com/en-us/news/technology/us-researchers-develop-unhackable-computer-chip-that-works-on-light/ar-BB1in4AY?ocid=socialshare&pc=ASTS&cvid=1256c5647de9453a9666fe00fb8faa13&ei=47\n",
    "text = \"\"\"Researchers at the University of Pennsylvania have developed a new computer chip that uses light instead of electricity. This could improve the training of artificial intelligence (AI) models by improving the speed of data transfer and, more efficiently, reducing the amount of electricity consumed.\n",
    "Humanity is building the exascale supercomputers today that can carry out a quintillion computations per second. While the scale of the computation may have increased, computing technology is still working on the principles that were first used in the 1960s.\n",
    "Researchers have been working on developing computing systems based on quantum mechanics, too, but these computers are at least a few years from becoming widely available if not more. The recent explosion of AI models in technology has resulted in a demand for computers that can process large sets of information. The inefficient computing systems, though, result in high consumption of energy.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.8756259083747864}]\n"
     ]
    }
   ],
   "source": [
    "# Text Classification\n",
    "classifier = pipeline(\"text-classification\")\n",
    "\n",
    "print(classifier(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'I-ORG', 'score': 0.9950655, 'index': 4, 'word': 'University', 'start': 19, 'end': 29}, {'entity': 'I-ORG', 'score': 0.9960758, 'index': 5, 'word': 'of', 'start': 30, 'end': 32}, {'entity': 'I-ORG', 'score': 0.9958604, 'index': 6, 'word': 'Pennsylvania', 'start': 33, 'end': 45}, {'entity': 'I-MISC', 'score': 0.9550863, 'index': 29, 'word': 'AI', 'start': 181, 'end': 183}, {'entity': 'I-MISC', 'score': 0.97647524, 'index': 142, 'word': 'AI', 'start': 767, 'end': 769}]\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition\n",
    "ner = pipeline(\"ner\")\n",
    "print(ner(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.7762266397476196, 'start': 927, 'end': 953, 'answer': 'high consumption of energy'}\n"
     ]
    }
   ],
   "source": [
    "# Question Answering\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "question = \"Why the current computing systems are inefficient?\"\n",
    "print(question_answerer(question=question, context=text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Your min_length=56 must be inferior than your max_length=50.\n",
      "c:\\Users\\krish\\miniconda3\\envs\\llmsys\\lib\\site-packages\\transformers\\generation\\utils.py:1158: UserWarning: Unfeasible length constraints: `min_length` (56) is larger than the maximum possible length (50). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': ' Researchers at the University of Pennsylvania have developed a new computer chip that uses light instead of electricity. This could improve the training of artificial intelligence (AI) models by improving the speed of data transfer and, more efficiently, reducing the amount of'}]\n"
     ]
    }
   ],
   "source": [
    "# Summarization\n",
    "summarizer = pipeline(\"summarization\")\n",
    "print(summarizer(text, max_length=50, clean_up_tokenization_spaces=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "source.spm: 100%|██████████| 789k/789k [00:00<00:00, 7.66MB/s]\n",
      "c:\\Users\\krish\\miniconda3\\envs\\llmsys\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\krish\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-ro. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "target.spm: 100%|██████████| 817k/817k [00:00<00:00, 9.47MB/s]\n",
      "vocab.json: 100%|██████████| 1.39M/1.39M [00:00<00:00, 19.3MB/s]\n",
      "c:\\Users\\krish\\miniconda3\\envs\\llmsys\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'Cercetătorii de la Universitatea din Pennsylvania au dezvoltat un nou cip de calculator care utilizează lumina în loc de electricitate. Acest lucru ar putea îmbunătăți formarea modelelor de inteligență artificială (AI) prin îmbunătățirea vitezei transferului de date și, mai eficient, reducerea cantității de energie electrică consumată. Umanitatea construiește supercomputer-urile exascale astăzi, care pot efectua un cvintilioane de calcule pe secundă. În timp ce scara de calcul poate să fi crescut, tehnologia de calcul este încă de lucru pe principiile care au fost utilizate pentru prima dată în anii 1960. Cercetătorii au lucrat la dezvoltarea sistemelor de calcul bazate pe mecanica cuantică, de asemenea, dar aceste calculatoare sunt de cel puțin câțiva ani de la a deveni disponibile pe scară largă, dacă nu mai mult. Explozia recentă a modelelor AI în tehnologie a dus la o cerere de calculatoare care pot procesa seturi mari de informații. Sistemele de calcul ineficiente, deși, conduc la un consum ridicat de energie.'}]\n"
     ]
    }
   ],
   "source": [
    "# Translation\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-ro\")\n",
    "outputs = translator(text, min_length=60, clean_up_tokenization_spaces=True)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Researchers at the University of Pennsylvania have developed a new computer chip that uses light instead of electricity. This could improve the training of artificial intelligence (AI) models by improving the speed of data transfer and, more efficiently, reducing the amount of electricity consumed.\n",
      "Humanity is building the exascale supercomputers today that can carry out a quintillion computations per second. While the scale of the computation may have increased, computing technology is still working on the principles that were first used in the 1960s.\n",
      "Researchers have been working on developing computing systems based on quantum mechanics, too, but these computers are at least a few years from becoming widely available if not more. The recent explosion of AI models in technology has resulted in a demand for computers that can process large sets of information. The inefficient computing systems, though, result in high consumption of energy.\n",
      "\n",
      "\n",
      " You are right :\n",
      "Yes, the new computer chip is a great innovation. But, it will increase the time for machines to\n"
     ]
    }
   ],
   "source": [
    "# Text Generation\n",
    "generator = pipeline(\"text-generation\")\n",
    "response = \"Yes, the new computer chip is a great innovation.\"\n",
    "prompt = text + \"\\n\\n You are right :\\n\" + response\n",
    "outputs = generator(prompt, max_length=200)\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
